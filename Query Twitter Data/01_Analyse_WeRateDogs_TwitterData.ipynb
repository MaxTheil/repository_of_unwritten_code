{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs - Query Twitter Data\n",
    "<ul>\n",
    "<li><a href=\"#intro\">I. - Introduction</a></li>\n",
    "<li><a href=\"#gathering\">II. - Data Gathering</a></li>\n",
    "<li><a href=\"#assessment\">III. - Data Assessment</a></li>\n",
    "<li><a href=\"#wrangling\">IV. - Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">V. - Explanatory Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">VI. - Conclusions</a></li>\n",
    "<li><a href=\"#references\">VII. - References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## I. - Introduction\n",
    "\n",
    "During the course of my Udacity \"Data Analyst\" Nanodegree I analysed Tweets from [WeRateDogs®](https://twitter.com/dog_rates?lang=eng). WeRateDogs® shows off dog picutes in all variations and consider itself as the only source for professional dog ratings. As of March 2020, over 8,7 Mio. twitter accounts follow the supplier of cute doggo pictures.\n",
    "\n",
    "This report aims to answer simple and important questions of online marketing: \n",
    "##### 1. When is the best time for a tweet?\n",
    "To be precise, we want to analyze if tweets tweeted during the weekend are more popular than tweets during workdays. The same analysis will be done for hours. Since \"popularity\" is barely quantifiable # of retweets and # of favorites is used instead.\n",
    "\n",
    "##### 2. Do some breeds outperform others?\n",
    "Do some breeds receive a significant higher popularity and if so which breed is the most popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## II. - Data Gathering\n",
    "\n",
    "#### II. A.) Importing Packages\n",
    "The most important packages were imported including Pandas, Numpy and Matplotlib. In addition, the packages Tweepy, Requests and JSON are needed to query data from twitter. Datetime, Random and Image are optional for the project itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import tweepy\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. B.) Loading CSV Data\n",
    "Afterwards, the CSV was loaded into the notebook and I had a first look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-archive-enhanced.csv', index_col=['tweet_id'], parse_dates=['timestamp','retweeted_status_timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some column names aren't intuitive. Hence, we check their meanings according to the [data dictionary](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object).\n",
    "\n",
    "**The key takeaways are:**\n",
    "\n",
    "- in_reply_to_status_id and in_reply_to_user_id are filled when the twee was a reply to another tweet.\n",
    "- retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp represents the original tweet data in case of a retweet.\n",
    "- doggo, floofer, pupper and puppo have been added by the UdaCity team but are messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. C.) Query Twitter Data\n",
    "\n",
    "In the next steps, the twitter API was used to gather additional information about each of the given tweets. Therefore Tweepy was used to gather tweet data and write each tweets data into a JSON-file. The correct tweets are identified by their respective tweet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = 'qqq'\n",
    "access_secret = 'qqq'\n",
    "account_key = 'qqq'\n",
    "account_secret = 'qqq'\n",
    "\n",
    "auth = tweepy.OAuthHandler(account_key, account_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication successful\")\n",
    "except:\n",
    "    print(\"Authentication error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, tweet data is gathered by Tweepy. In case of an error, the respective tweet id and the error message is written into an CSV. In addition, datetime package was used to measure runtime of the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.today()\n",
    "\n",
    "tweet_data = []\n",
    "tweep_errors = []\n",
    "\n",
    "id_list = list(df.index)\n",
    "\n",
    "for x in id_list:    \n",
    "    try:\n",
    "        tweet_data.append(api.get_status(x, tweet_mode='extended')._json)\n",
    "    except tweepy.TweepError as error:\n",
    "        print('Unable to query:', str(x))\n",
    "        tweep_errors.append({'id':x, 'error':error})\n",
    "        \n",
    "print('Summary: '+str(len(tweep_errors))+' Missing Tweets ('+str(round(len(tweep_errors)/len(tweet_data),2))+'%)')\n",
    "\n",
    "with open('tweet_json.txt', 'w') as x:\n",
    "    json.dump(tweet_data, x)\n",
    "    \n",
    "pd.DataFrame(tweep_errors).to_csv('tweet_errors.csv', index=False)\n",
    "    \n",
    "print(str('Duration: '+str(datetime.datetime.today() - start_date)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are 25 errors i.e. these tweets are missing when merging datasets, which needs further investigation.\n",
    "\n",
    "**Before the tweet data can be used, is was necesarry to flatten the JSON-file**, since tweet objeects are stored in nested JSON-files. The respective code snipped was takes from [TowardsDataScience](https://towardsdatascience.com/how-to-flatten-deeply-nested-json-objects-in-non-recursive-elegant-python-55f96533103d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_json.txt', 'r') as x:\n",
    "    load_data = json.load(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(x):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(y, name=''):\n",
    "        if type(y) is dict:\n",
    "            for z in y:\n",
    "                flatten(y[z], name + z + '_')\n",
    "        elif type(y) is list:\n",
    "            i = 0\n",
    "            for z in y:\n",
    "                flatten(z, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = y\n",
    "\n",
    "    flatten(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(load_data)):\n",
    "    load_data[x] = flatten_json(load_data[x])\n",
    "    \n",
    "df_add = pd.DataFrame(load_data).set_index('id')\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the diffrence in rows between the two datasets is explained by the 25 errors. \n",
    "\n",
    "Since there are 911 columns currently, columns that are not needed for analysis will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['full_text','extended_entities_media_0_media_url_https','source','in_reply_to_status_id','in_reply_to_user_id','user_followers_count','user_favourites_count','retweet_count','favorite_count','retweeted','is_quote_status']\n",
    "\n",
    "df_add = df_add[columns]\n",
    "df_add.rename(columns={'full_text':'text','extended_entities_media_0_media_url_https':'expanded_urls'}, inplace=True)\n",
    "\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. D.) Breed Prediction Data\n",
    "\n",
    "Third source for this project is the image-predictions dataset provided by Udacity in a TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('image-predictions.tsv', sep='\\t', index_col='tweet_id')\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks clean so far, thus it can be joined to the rest of the data. However, the breed prediction dataset has fewer observation compared to the other datasets. Therefore, we need to use a left join when merging to keep track of missing and other errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. E.) Merging Data\n",
    "\n",
    "All three datasets were merged together by a pandas left join. A left join was used for merging since we dont want to loose any information - e.g. an inner join would loose the 25 error tweets. The same is true for dog breed prediction data. Nevertheless, missings have to be imputed in data wrangling chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_pre = df.join(df_add, how='left', rsuffix='_add')\n",
    "df_combined_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_pre_c = df_combined_pre.copy()\n",
    "\n",
    "df_combined = df_combined_pre_c.join(df_pred, how='left', rsuffix='_pred')\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, there seem to be no reason for major concern. However, data quality checks are performed in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. F.) Data Quality Checks\n",
    "\n",
    "Last but not least, we checked the quality of the merged dataset by comparing duplicated columns and cheking our errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = pd.read_csv('tweet_errors.csv')\n",
    "df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check of the errors revealed, that almost all errors occured due to code 144 i.e. the tweets have been deleted. Only one tweet raised an error because my account is not authorized to see the status. Overall, the number of missings is immaterial and will not bias the following analysis.\n",
    "\n",
    "To check the quality of our first join, text column was used because the column is filled with unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[df_combined['text'] != df_combined['text_add']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of any mismatch .min() would evaluate to 0 (False)\n",
    "(df_combined[df_combined['text'] != df_combined['text_add']].index ==  df_errors['id']).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a mismatch in texts but this discrepancy can be fully explained by the 25 errors. Therefore, we conclude that our first join is correct and - more important - that **observations match**.\n",
    "\n",
    "For evaluation of the second join, expanded_urls_add and jpg_url was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select mismatches\n",
    "df_combined[df_combined['expanded_urls_add'] != df_combined['jpg_url']][['expanded_urls_add','jpg_url']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_combined[df_combined['expanded_urls_add'] != df_combined['jpg_url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select mismatches where only 1 image exists  \n",
    "df_combined[(df_combined['expanded_urls_add'] != df_combined['jpg_url'])&(df_combined['img_num']==1)][['expanded_urls_add','jpg_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = df_combined[(df_combined['expanded_urls_add'] != df_combined['jpg_url'])&(df_combined['img_num']==1)].index\n",
    "\n",
    "# Check if any observations in checlist is at the same time an error\n",
    "any(x in df_errors.id for y in checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data suggests a mismatch of 589 rows. A closer look reveals that there are two types of mismatches. First, both URL's refer to the same tweet but diffrent pictures - an example is provided below.  Second, \"expanded_urls_add\" is missing, while jpg_url isn't.\n",
    "\n",
    "As seen before there can be a marginal mismatch, however 13 mismatches dont give raise to major concern. Therefore, we conclude that overall the join worked and number of mismatches is within a tolerable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://pbs.twimg.com/media/DFDw2tsUAAEw7XW.jpg', embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://pbs.twimg.com/media/DFDw2tyUQAAAFke.jpg', embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessment'></a>\n",
    "## III. - Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogramms were plotted in addition to the describe-command in order to gain a better understanding of the data. The below plots show that retweet count and favorite count are highly skewed. This is not a problem per se but it has to be considered when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list = ['rating_numerator','rating_denominator','retweet_count','favorite_count']\n",
    "\n",
    "for x in hist_list:\n",
    "            df_combined[x].hist();\n",
    "            plt.title(str(x).upper())\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "            plt.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index was checked instead of the row, since tweets should be unique \n",
    "df_combined.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check for duplicates was done, and shows a negative result i.e. there aren't any duplictated rows.\n",
    "\n",
    "**Summary**\n",
    "1. As can be seen, in_reply_to_status_id and in_reply_to_user_id contain mostly missings. The same is true for retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp. However, this makes sence, since WeRateDogs® tweets new picutres instead of replies.  Retweets and Replies have to be cleaned, since we only care about original dog ratings.\n",
    "2. In addition to that, there are some missing URLS's as well as diffrence between expanded_urls (2297), expanded_urls_add (2059) and jpg_url (2075). Hence, further investigation is necessary, although not mandatory because images dont play a major role in our analysis.\n",
    "3. Dates are formatted as strings instead of datetime objects (Note: this has been fixed by parsing \"parse_dates\" in the \"pd.read_csv\"-command)\n",
    "4. in_reply_to_status_id and in_reply_to_user_id are formatted as floats instead of strings. These columns are filled with numbers but these numbers are unique identifiers. Therefore, they cannot be used for calculation and will be converted to string.\n",
    "5. retweet_count, favorite_count, user_followers_count and user_favourites_count are formatted as floats instead of integers. For convenience these columns will be converted to integers.\n",
    "6. The source columns contains full HTML tags. However, the tweet source should be something like \"Web\" or \"Smartphone\" - which can be found in between the HTML tags. Hence, the correct information needs to be extracted.\n",
    "7. Columns doggo, pupper and puppo are reflect the same information, namely age of the dog.\n",
    "8. rating_denominator can be smaller than 1 and even be 0. This is a problem for calculation of the final rating, since division by 0 will result in and error. In case you divide a number by something between 0 and 1 the result is an multiplication, which doesn't make sense.\n",
    "9. The final rating is reflected in two columns rating numerator and rating denominator. Since, final rating will be used in the analysis we need to combine both fields, which is related to the previous problem.\n",
    "10. Due to the left joins there are missing values for retweet_count, favorite_count, user_followers_count and user_favourites_count which have to be imputed. For completeness, please note that retweeted and is_quote_status also contains missing, but these columns are dropped anyways.\n",
    "11. The same holds true for dog breed predictions. Nonetheless, it might be difficult to replace missing values.\n",
    "12. To answer our first question a weekday feature must be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dog = random.choice(df_combined[df_combined.expanded_urls_add.notnull()].expanded_urls_add.values)\n",
    "Image(url=random_dog, embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## IV. - Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_c = df_combined.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 1.) Retweets and replies\n",
    "\n",
    "Since we are only interested in original dog rating and no duplicates retweets, replies and quotes will be dropped - since retwees reflect another kind of duplicates, which wasn't not detected by out check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with retweet_status_id\n",
    "df_combined_clean = df_combined_c.loc[df_combined_c['retweeted_status_id'].isna()==True]\n",
    "df_combined_clean['retweeted_status_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with is_quote_status\n",
    "df_combined_cleaned = df_combined_clean.loc[df_combined_clean['is_quote_status']==False]\n",
    "df_combined_cleaned['is_quote_status'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 2.) Missing URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select missings\n",
    "df_combined_cleaned[df_combined_cleaned['expanded_urls'].isna()]\n",
    "# Select missings + replies\n",
    "df_combined_cleaned[(df_combined_cleaned['expanded_urls'].isna())&(df_combined_cleaned['in_reply_to_status_id'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check revealed, that missing URLs are replies.\n",
    "\n",
    "#### IV. 4.) Formates - Floats to Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_strings = ['in_reply_to_status_id','in_reply_to_user_id','in_reply_to_status_id_add','in_reply_to_user_id_add','retweeted_status_id','retweeted_status_user_id']\n",
    "\n",
    "for x in convert_strings:\n",
    "    df_combined_cleaned[x] =  df_combined_cleaned[x].apply(lambda y: str(y))\n",
    "    print(str(x)+': '+str(df_combined_cleaned[x].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 5.) Formates - Floats to Integers\n",
    "\n",
    "These columns are clearly numeric values that could be used for calcultion.\n",
    "\n",
    "At the same time median values were imputed for missings, since we already called the correct objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ints = ['retweet_count','favorite_count','user_followers_count','user_favourites_count']\n",
    "\n",
    "for x in convert_ints:\n",
    "    df_combined_cleaned[x].fillna(df_combined_cleaned[x].median(), inplace=True)\n",
    "    df_combined_cleaned[x] =  df_combined_cleaned[x].apply(lambda y: int(y))\n",
    "    print(str(x)+': '+str(df_combined_cleaned[x].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 6.) Source column\n",
    "\n",
    "Since this column contains full HTML tags, it was decided to split the string by bracktes. Thus, we receive the information between the HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned['source'] =  df_combined_cleaned['source'].apply(lambda y: y.split('>')[1].split('<')[0])\n",
    "df_combined_cleaned['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 7.) Age - doggo, puppo, pupper\n",
    "\n",
    "As mentioned before, doggo, floofer, pupper and puppo are slang terms for dogs. A dog is mapped to the respective category by age. Hence, the true information in all three columns is the same - age.\n",
    "\n",
    "Thus we combine the information doggo, puppo and pupper in a new categorical column called age.\n",
    "\n",
    "Before merging doggo, puppo and pupper it was checked that columns dont overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where pupper is true and check wether puppo is empty and vice versa  \n",
    "(df_combined_cleaned[df_combined_cleaned['pupper']=='pupper'].puppo != 'None').sum() == (df_combined_cleaned[df_combined_cleaned['puppo']=='puppo'].pupper != 'None').sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where pupper is true and check wether doggo is empty and vice versa\n",
    "(df_combined_cleaned[df_combined_cleaned['pupper']=='pupper'].doggo != 'None').sum() == (df_combined_cleaned[df_combined_cleaned['doggo']=='doggo'].pupper != 'None').sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned[(df_combined_cleaned['pupper']=='pupper')&(df_combined_cleaned['doggo']=='doggo')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where doggo is true and check wether puppo is empty and vice versa\n",
    "(df_combined_cleaned[df_combined_cleaned['doggo']=='doggo'].puppo != 'None').sum() == (df_combined_cleaned[df_combined_cleaned['puppo']=='puppo'].doggo != 'None').sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since columns overlap, merging is not possible without loosing information. In addition, these columns are optional for our analysis. Hence, they were dropped in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.drop(['doggo','pupper','puppo','floofer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 8.) Rating denominator\n",
    "\n",
    "Rating is calculated in the next step. Before calculation, rating denominators were floored at 1 to prevent division by 0 and amplification in case a denominator is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.loc[df_combined['rating_denominator']<=0, 'rating_denominator'] = 1\n",
    "(df_combined_cleaned['rating_denominator']<=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 9.) Final rating\n",
    "\n",
    "The final dog ratings consist of two elements a numerator and a denominator. Since the denominator can change it would be misleading to compare dogs based on the numerator. In case of a constant denominator there is no diffrence between the final rating and the numerator, since the rank ordering would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned['rating'] = df_combined_cleaned['rating_numerator'] / df_combined_cleaned['rating_denominator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned['rating'].hist()\n",
    "plt.title(str('rating').upper())\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "plt.clf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_combined_cleaned[df_combined_cleaned['rating']>df_combined_cleaned['rating'].quantile(0.99)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogramm above is biased due to 14 outliers with a rating greater than the 99% confidence intervall. Thus, values were capped at the 99% confidence intervall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.loc[df_combined_cleaned['rating']>df_combined_cleaned['rating'].quantile(0.99), 'rating'] = df_combined_cleaned['rating'].quantile(0.99)\n",
    "\n",
    "df_combined_cleaned['rating'].hist()\n",
    "plt.title(str('rating').upper())\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "plt.clf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.drop(['rating_numerator','rating_denominator'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. 10.) Missing imputation\n",
    "\n",
    "retweet_count, favorite_count, user_followers_count and user_favourites_count have already been imputed in IV. 5.).\n",
    "\n",
    "#### IV. 11.) Missing dog breeds\n",
    "\n",
    "Due to the left join used before, there are missing dog breeds. Nonetheless, it is quite difficult to impute 152 values by median or any other concept, since it's a categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing ratio: '+str(round(df_combined_cleaned.p1.isna().sum() / len(df_combined_cleaned)*100,2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are to many missing to drop them. Therefore, it was decided to keep these observations for the first question, but to omit them for the third question.\n",
    "\n",
    "#### IV. 12.) Weekday feature\n",
    "\n",
    "To anser our first question it is important to know the exact weekday of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned['weekday'] = df_combined_cleaned['timestamp'].apply(lambda x: x.date().weekday()+1)\n",
    "df_combined_cleaned['weekday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "Overall, problems were solved. However, some features will not be used in the final analysis, since their use might lead to unreliable results due to marginal data. \n",
    "\n",
    "Finally, columns related to retweets, quotes and replies can therefore be dropped. In addition, columns that will not be used e.g. img_num in the analysis are dropped as well columns used for quality checks - expanded_urls_add is preferred over jpg_url, due to less missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.drop(['in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp','text_add','source_add','in_reply_to_status_id_add','in_reply_to_user_id_add','retweeted','is_quote_status','img_num','jpg_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_cleaned.to_csv('cleaned_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dog = random.choice(df_combined[df_combined.expanded_urls_add.notnull()].expanded_urls_add.values)\n",
    "Image(url=random_dog, embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## V. - Explanatory Analysis\n",
    "\n",
    "WeRateDogs® is a popular twitter blog, which tweets on a daily basis. A very important question for each blogger is: What drives my popularity?\n",
    "\n",
    "In the following analysis we want to dive into the number and answer simple but important questions. Popularity is measured by number of favorites or - more important - number of retweets. [Why are retweets more important?](https://medium.com/@Encore/favorites-vs-retweets-and-why-one-is-more-important-than-the-other-ba12ee20e9ba) For these reasons we will focus on number of retweets in this analysis.\n",
    "\n",
    "\n",
    "**1. When is the best time for a tweet?**\n",
    "To be precise, we want to analyze if tweets tweeted during the weekend are more popular than tweets during workdays. The same analysis will be done for hours. Since \"popularity\" is barely quantifiable # of retweets and # of favorites is used instead.\n",
    "\n",
    "**2. Do some breeds outperform others?**\n",
    "Do some breeds receive a significant higher popularity and if so which breed is the most popular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('cleaned_data.csv', index_col=['tweet_id'], parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. When is the best time for a tweet?\n",
    "\n",
    "In order to answer our first question, line plots were drawn below. The plots show the weekdays on their x-axis, starting with Monday (1), till Sunday (7). The number of tweets is plotted on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggr_weekday_sum = df_analysis.groupby('weekday').retweet_count.sum()\n",
    "\n",
    "aggr_weekday_sum.plot();\n",
    "plt.grid(which='major', axis='both')\n",
    "plt.title('Total number of retweets grouped by day')\n",
    "plt.ylabel('Number of retweets')\n",
    "plt.xlabel('Weekday')\n",
    "plt.show()\n",
    "plt.clf;\n",
    "\n",
    "aggr_weekday_max = df_analysis.groupby('weekday').retweet_count.max()\n",
    "\n",
    "aggr_weekday_max.plot();\n",
    "plt.grid(which='major', axis='both')\n",
    "plt.title('Maximum number of retweets grouped by day')\n",
    "plt.ylabel('Number of retweets')\n",
    "plt.xlabel('Weekday')\n",
    "plt.show()\n",
    "plt.clf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first plot, we see that the best day for tweeting is Wednesday, followed by Monday. This is interesting because my first guess would have been saturday, since people have more free time compared to working days. On the other hand, Saturday is the day most popular tweet overall, as can be seen in the second plot. Hence, we checked the retweet volatility in the next two plots,to validate these result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.plot.scatter(x='weekday', y='retweet_count')\n",
    "plt.grid(which='major', axis='both')\n",
    "plt.title('Number of retweets grouped by day')\n",
    "plt.ylabel('Number of retweets')\n",
    "plt.xlabel('Weekday')\n",
    "plt.show()\n",
    "plt.clf;\n",
    "\n",
    "df_analysis.plot.scatter(x='weekday', y='retweet_count')\n",
    "plt.grid(which='major', axis='both')\n",
    "plt.title('Number of retweets grouped by day')\n",
    "plt.ylabel('Number of retweets')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylim([0,25000])\n",
    "plt.show()\n",
    "plt.clf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot suggests that volatility decreases. However, if we remove outliers the story is diffrent. In the second plot we see that tweets during weekdays perform better than tweets during weekends. Moreover, volatility seems to be almost constant.\n",
    "\n",
    "**To check the results, an A|B test was performed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_df = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    weekday_df.append({'weekdays':df_analysis.sample(2500, replace=True).query('weekday<6').retweet_count.mean(),\n",
    "               'weekends':df_analysis.sample(2500, replace=True).query('weekday>5').retweet_count.mean(),\n",
    "               'mean_diff':df_analysis.sample(2500, replace=True).query('weekday<6').retweet_count.mean() - df_analysis.sample(2500, replace=True).query('weekday>5').retweet_count.mean()})\n",
    "    \n",
    "weekday_df = pd.DataFrame(weekday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weekday_df['mean_diff'], alpha = 0.5, label='weekdays')\n",
    "plt.axvline(weekday_df['mean_diff'].mean(), color='darkred')\n",
    "\n",
    "plt.title('Average diffrence in number of retweets grouped by weekday/weekend')\n",
    "plt.xlabel=('Average number of retweets')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, which='major', axis='both')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Mean diffrence: '+str(weekday_df['mean_diff'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A|B test contradicts our previous results and shows that there is no statistical significant diffrence in number of retweets - and by far no practical significance. Thus, we conclude that the day doens't matter when tweeting. **It seems like tweets spread over time, an information not covered in our analysis - future research has to proove this hypothesis.**\n",
    "\n",
    "##### 2. Do some breeds outperform others?\n",
    "Is there a dog breed, which is more popular in general and lead to more popular tweets. This question is answered by the following bar plot, which shows the total number of retweets per breed for the five most popular breeds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_pred = df_analysis.groupby('p1').retweet_count.sum().nlargest(5)\n",
    "\n",
    "aggr_pred.plot.bar();\n",
    "plt.grid(which='major', axis='both')\n",
    "plt.title('Total number of retweets grouped by breed - 5 largest')\n",
    "plt.ylabel('Total number of retweets')\n",
    "plt.xlabel('Breed')\n",
    "plt.show()\n",
    "plt.clf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most popular breeds are:**\n",
    "\n",
    "| Rank | Dog breed |\n",
    "| :--- | :--- |\n",
    "| 1 | Golden Retriever |\n",
    "| 2 | Labrador Retriever |\n",
    "| 3 | Welsh Corgi Pembroke |\n",
    "| 4 | Chihuahua |\n",
    "| 5 | Samoyed |\n",
    "\n",
    "**Our data supports the cliche of the dolgen retriever as the most popular breed.** Thus, we conclude that tweets featuring golden retriever are more likely to get retweeted and become more popular. Nevertheless, it is wrong for WeRateDogs® to tweet golden retrievers only. What makes a blog interesting in the long run is change and diversity. But every now and then WeRateDogs® should think about tweeting golden retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_retriever = df_analysis[df_analysis['p1']=='golden_retriever']\n",
    "\n",
    "random_retriever = random.choice(golden_retriever[golden_retriever.expanded_urls_add.notnull()].expanded_urls_add.values)\n",
    "Image(url=random_retriever, embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## VI. - Conclusions\n",
    "\n",
    "The above analysis made three points clear:\n",
    "\n",
    "**1. There is no significant diffrence between tweeting during working week or tweeting during the weekend.**\n",
    "\n",
    "**2. Tweets featuring Golden Retriever perform on average better than tweets featuring other breeds.**\n",
    "\n",
    "**3. However, every blog should offer diversity - but every now and then a golden retriever!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_retriever = df_analysis[df_analysis['p1']=='golden_retriever']\n",
    "\n",
    "random_retriever = random.choice(golden_retriever[golden_retriever.expanded_urls_add.notnull()].expanded_urls_add.values)\n",
    "Image(url=random_retriever, embed=True, width=260, height=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## VII. - References\n",
    "\n",
    "\n",
    "- [Writing to a JSOn file](https://stackabuse.com/reading-and-writing-json-to-a-file-in-python/)\n",
    "\n",
    "\n",
    "- [Tweepy Error Messages](https://www.programcreek.com/python/example/13279/tweepy.TweepError)\n",
    "\n",
    "\n",
    "- [Flatten Nested JSON files](https://towardsdatascience.com/how-to-flatten-deeply-nested-json-objects-in-non-recursive-elegant-python-55f96533103d)\n",
    "\n",
    "\n",
    "- [Why are retweets more important?](https://medium.com/@Encore/favorites-vs-retweets-and-why-one-is-more-important-than-the-other-ba12ee20e9ba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
